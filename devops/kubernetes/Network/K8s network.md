# K8S Network

쿠버네티스 네트워크에 대한 것들을 정리하는 시간을 가져보자. (https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/)



## 컨테이너 간 네트워킹

보통 VM에서의 네트워크 통신은 이더넷 장치와 직접 상호작용하는 것으로 본다. 

![image-20211222174206872](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/root-namespace.png)

리눅스에서 사실 각 구동 중인 프로세스들은 자체 라우트, 방화벽 규칙 및 네트워크 장치가 있는 논리적인 네트워킹 스택을 제공하는 네트워크 네임스페이스 내에서 통신한다. 본질적으로 네트워크 네임스페이스는 네임스페이스 내의 모든 프로세스에 대해 완전히 새로운 네트워크 스택을 제공한다.



도커 구조의 관점에서 보면, 파드는 네트워크 네임스페이스를 공유하는 도커 컨테이너의 그룹으로서 모델링된다. 파드 내 컨테이너들은 모두 파드에 할당된 네트워크 네임스페이스를 통해 할당된 동일한 IP 주소와 포트 공간을 가지며 각 컨테이너들은 같은 네임스페이스에 있는 컨테이너들과 `localhost` 로 통신할 수 있다. 

VM에서의 각 파드에 대한 네트워크 네임스페이스를 생성할 수 있다. 이는 앱 컨테이너가 도커의 `-net=container:` 기능을 사용하여 해당 네임스페이스를 조인하는 동안 네트워크 네임스페이스를 열린 상태로 유지하는 파드 컨테이너로 도커를 사용하여 구현된다. 아래 그림은 각 파드가 공유 네임스페이스 안에서 여러 도커 컨테이너로 구성되는 방식을 보여준다.

![image-20211222174206872](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-namespace.png)







리눅스에서는 다음 명령어를 통해 네임스페이스를 생성하고 조회할 수 있다.

```shell
$ ip netns add ns1

$ ip netns
ns1
```



하지만 쿠버네티스 환경에서는 위의 명령어로 네임스페이스가 나오지 않는다. 파드의 네임스페이스를 조회하기 위해서는 아래 절차를 밟아야 한다.

1. `docker ps` 로 조회하고자 하는 CONTAINER_ID를 알아야 한다. 쿠버네티스 파드의 네임스페이스를 생성하고 유지하는 컨테이너는 `pause` 컨테이너이다. 

   ```shell
   $ docker ps | grep pause
   ```

2. `nsenter` 명령어를 이용하여 네트워크 네임스페이스의 자원을 볼 수 있다.

   ```shell
   $ sudo nsenter -t 1763608 -n ip addr
   1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
       link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
       inet 127.0.0.1/8 scope host lo
          valid_lft forever preferred_lft forever
   2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
       link/ipip 0.0.0.0 brd 0.0.0.0
   4: eth0@if133: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1434 qdisc noqueue state UP group default
       link/ether fe:c0:b1:df:02:43 brd ff:ff:ff:ff:ff:ff link-netnsid 0
       inet 10.244.18.61/32 brd 10.244.18.61 scope global eth0
          valid_lft forever preferred_lft forever
   ```

   



## 파드간 네트워킹

쿠버네티스에서 모든 파드는 실제 IP 주소를 가지고있으며 각 파드는 IP 주소를 사용하여 다른 파드와 통신한다. 이 과정은 파드가 클러스터에서 노드의 위치에 상관없이 쿠버네티스가 실제 IP를 사용하여 파드간 통신을 가능하게 하는 방법을 이해하기 위함이다. 

일단 내부 네트워크를 통해 노드 간에 통신하는 복잡성을 피하기 위해 동일한 머신에 있는 파드 끼리의 네트워킹을 고려해보자.

파드의 관점에서 보면, 같은 노드 내 다른 네임스페이스와 통신해야 하는 자체 이더넷 네임스페이스에 존재한다. 다행히 여러 네임스페이스에 분산될 수 있는 두 개의 가상 인터페이스로 구성된 리눅스 가상 이더넷 장치 또는 *veth pair* 를 사용하여 네임스페이스를 연결할 수 있다. 파드 네임스페이스를 연결하기 위해서는 veth 쌍의 한쪽을 루트 네트워크 네임스페이스에 할당하고 다른 쪽을 파드의 네트워크 네임스페이스에 할당할 수 있다. 각 veth 쌍은 패치 케이블처럼 작동하여 양측을 연결하고 트래픽이 둘 사이를 흐르게 한다. 

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-veth-pairs.png)



이 관점에서, 파드 각각이 자신의 이더넷 장치와 IP 주소를 가지고 있다고 믿도록 각각 하나의 네트워크 네임스페이스를 갖도록 파드를 설정했으며 노드의 루트 네임스페이스에 연결되었다. 이제, 파드가 루트 네임스페이스를 통해 다른 파드와 통신하기를 원하고  이를 위해 네트워크 브릿지를 사용한다.

리눅스 이더넷 브릿지는 두 개 이상의 네트워크 세그멘트를 통합하는데 사용되는 가상 L2 네트워크 장치로, 두 네트워크를 함께 연결하기 위해 투명하게 작동한다. 브릿지는 데이터 패킷의 목적지를 검사하고 브릿지에 연결된 다른 네트워크 세그먼트에 패킷을 전달할지 여부를 결정하여 출발지와 도착지 사이의 포워딩 테이블을 유지함으로써 작동한다. 브릿징 코드는 네트워크의 각 이더넷 장치에 고유한 MAC 주소를 보고 데이터를 브릿지할지 아니면 삭제할지를 결정한다.

브릿지는 주어진 IP 주소와 관련된 link-layer MAC 주소를 검색하기 위해 ARP 프로토콜을 구현한다. 데이터 프레임을 브릿지에서 받을 때, 브릿지는 프레임을 모든 연결된 장치로 브로드캐스트하고, 프레임에 응답하는 장치는 조회 테이블(lookup table)에 저장된다. 동일한 IP 주소를 가진 향후 트래픽은 조회 테이블을 사용하여 패킷을 전달할 올바른 MAC 주소를 찾는다.

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pods-connected-by-bridge.png)





### 같은 노드의 파드간 패킷 전달

각 파드가 자신의 네트워킹 스택으로 격리하는 네트워크 네임스페이스와 각 네임스페이스에서 루트 네임스페이스로 연결하는 가상 이더넷 장치와 네임스페이스를 함께 연결하는 브릿지를 통해 같은 노드에서 파드 간 트래픽을 전달할 수 있다.

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-to-pod-same-node.gif)



위의 그림에서 Pod1은  파드의 기본 장치로 사용할 수 있는 자신의 이더넷 장치인 `eth0` 에 패킷을 보낸다. Pod1에 대하여, `eth0` 은 가상 이더넷 장치를 통해 루트 네임스페이스의 `veth0`에 연결된다. 브릿지 `cbr0` 은 연결된 네트워크 세그먼트인 `veth0` 으로 구성된다. 패킷이 브릿지에 도달할 때, 브릿지는 ARP 프로토콜을 사용하여 패킷을 `veth1` 로 보낼 올바른 네트워크 세그먼트를 확인한다. 패킷이 가상 장치인 `veth1` 에 도달하면, 패킷은 Pod2의 네임스페이스와 해당 네임스페이스의 `eth0` 장치로 직접 전달된다. 

트래픽 흐름을 통해, 각 파드는 `localhost` 의  `eth0` 과만 통신하며 트래픽은 올바른 파드로 라우팅된다. 

쿠버네티스의 네트워킹 모델은 노드 전체에서 해당 IP 주소로 파드에 연결할 수 있어야 한다고 명시하고 있다. 즉, 파드의 IP 주소는 네트워크 내 다른 파드에서 항상 볼 수 있고, 각 파드는 자신의 IP 주소를 다른 파드가 보는 것과 동일하게 본다. 



### 다른 노드의 파드간 패킷 전달

서로 다른 노드에 있는 파드들 끼리 트래픽을 어떻게 라우팅하는지에 대해 알아보자. 쿠버네티스 네트워킹 모델에는 네트워크를 통해 파드IP에 연결할 수 있어야 한다. 하지만 어떻게 해야하는지는 지정하지 않는다. 사실, 이는 네트워크에 따라 다르지만 이를 더 쉽게 하기 위해 몇 가지 패턴이 설정된다.

일반적으로, 클러스터의 모든 노드에 해당 노드에서 구동하는 파드에 사용가능한 IP주소를 지정하기 위한 CIDR 블록이 할당된다. CIDR 블록으로 향하는 트래픽이 노드에 도달하면 트래픽을 올바른 포드로 전달하는 것은 노드의 책임이다. 아래 그림은 두 노드 사이의 트래픽 흐름을 나타낸다. (네트워크가 CIDR 블록의 트래픽을 올바른 노드로 라우팅할 수 있다고 가정)

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-to-pod-different-nodes.gif)



위의 그림은 Pod1에서 Pod4까지 패킷이 어떻게 전달되는지 보여준다.

먼저 패킷은 루트 네임스페이스의 가상 이더넷 장치와 쌍을 이루는 Pod1의 이더넷 장치를 통해 전송되면서 시작한다. 그리고 패킷은 루트 네임스페이스의 네트워크 브릿지에서 끝난다. 패킷에 대한 올바른 MAC 주소를 사용하여 브릿지에 연결된 장치(Pod4)가 없기 때문에 브릿지에서  ARP가 실패하기 때문이다. ARP가 실패하면, 브릿지는 패킷을 기본 라우트인 루트 네임스페이스의 `eth0` 장치로 보낸다. 이 시점에서 라우트는 노드를 떠나 네트워크에 진입한다. (여기서 네트워가 노드에 할당된 CIDR 블록을 기반으로 패킷을 올바른 노드로 라우팅할 수 있다고 가정한다.) 패킷은 목적지 노드의 루트 네임스페이스로 진입한다. (VM2의 `eth0`) 여기는 브릿지를 통해 올바른 가상 이더넷 장치로 라우팅된다. 마지막으로, Pod4의 네임스페이스에 있는 가상 이더넷 장치 쌍을 통과하여 라우트가 완료된다. 일반적으로, 각 노드는 내부에서 실행 중인 파드에 패킷을 전달하는 방법을 알고있다. 패킷이 목적지 노드에 도달한다면 패킷은 동일한 노드의 파드 간에 트래픽을 라우팅하는 것과 같은 방식으로 흐른다.



## 파드 - 서비스간 네트워킹

쿠버네티스 서비스를 생성하면 사용자를 대신하여 새로운 가상 IP가 생성된다. 클러스터 내부 어디든지 가상 IP로의 트래픽은 서비스와 연결된 파드 셋으로 로드밸런싱 될 것이다. 실제로 쿠버네티스는 서비스에 연결되어있는 건강한 파드로 트래픽을 분산시키는 분산 클러스터 내부 로드밸런서를 자동으로 생성하고 유지한다.



### NetFilter, Iptable

클러스터 내부에서 로드밸런싱을 수행하기 위해 쿠버네티스는 리눅스에 빌트인된 네트워크 프레임워크인 `netfilter` 에 의존한다. Netfilter는 리눅스에서 제공하는 프레임워크이며 다양한 네트워킹과 관련된 작업을 맞춤형 핸들러의 형태로 구현할 수 있도록 해준다. Netfilter는 패킷 필터링, 네트워크 주소 번역, 포트 번역과 관련된 여러 기능과 연산자를 제공해주며, 네트워크를 통해 패킷을 보내는 데 필요한 기능을 제공할 뿐 아니라 패킷이 컴퓨터 네트워크 내의 민감한 위치에 도달하는 것을 금지하는 기능을 제공해준다. 

`iptables`은 netfilter 프레임워크를 사용하여 패킷을 조작하고 변환하기 위한 규칙을 정의하기 위한 테이블 기반 시스템을 제공하는 유저스페이스 프로그램이다. 쿠버네티스에서 iptables 규칙은 쿠버네티스 apiserver에서 변경 사항을 감지하는 kube-proxy 컨트롤러가 구성한다. 서비스나 파드의 변경이 서비스의 가상 IP나 파드의 IP 주소를 업데이트하면 iptables 규칙이 업데이트되어 서비스로 향하는 트래픽을 파드로 올바르게 라우팅한다. iptables 규칙은 서비스의 가상 IP로 도착하는 트래픽을 감지하 일치할 경우, 사용가능한 파드 셋으로부터 임의의 파드 IP 주소가 선택되고 iptable 규칙은 패킷의 도착지 IP주소를 서비스의 가상 IP에서 선택된 파드 IP로 변경한다. 그리고 파드는 일회성이기 때문에 파드가 뜨거나 다운될 때 iptables 규칙이 업데이트되어 클러스터의 상태 변화를 반영한다. 

경로가 반환되면 IP 주소는 목적지 파드에서 나온다. 이 경우에서 iptables은 IP 헤더를 파드 IP에서 서비스 IP로 재작성한다. 그래서 파드는 전체 시간동안 서비스의 IP와만 통신했다고 생각한다.



### IPVS

쿠버네티스 버전 1.11부터 클러스터 내부 로드 밸런싱에 대한 옵션 중 IPVS이 추가되었다. IPVS*(IP Virtual Server)*는 netfilter 위에 구축되며 리눅스 커널의 부분으로써 transport-layer 로드밸런싱을 구현한다. IPVS는 호스트에서 실행되고 실제 서버 클러스터 앞에서 로드밸런서 역할을 하는 LVS *(Linux Virtual Server)* 에 통합된다. IPVS는 TCP 및 UDP 기반 서비스에 대한 요청을 실제 서버로 보낼 수 있으며 실제 서버의 서비스가 단일 IP 주소에서 가상 서비스로 나타나도록 한다. 따라서 IPVS는 쿠버네티스 서비스에 자연스럽게 적합한다.

쿠버네티스 서비스를 선언할 때, 클러스터 내부 로드밸런싱을 iptable로 할지 IPVS로 할지 지정할 수 있다. IPVS는 특히 로드밸런싱을 위해 설계되었으며 보다 효율적인 자료구조를 사용하여 iptables에 비해 거의 무제한 확장이 가능하다. IPVS로 로드밸런싱된 서비스를 생성할 때, 세가지 경우가 발생한다. 

1. 더미 IPVS 인터페이스거 노드에 생성된다.
2. 서비스의 IP주소가 더미 IPVS 인터페이스에 바인딩된다.
3. IPVS 서버는 각 서비스 IP 주소에 대해 생성된다.



추후에는 IPVS가 클러스터 내 로드밸런싱의 기본 방식이 될 수도 있다. 이러한 변경사항은 클러스터 내 로드밸런싱에만 영향을 미치며 클러스터 내 로드밸런싱을 위해 Iptables을 IPVS로 안전하게 교체할 수 있다.



### 파드에서 서비스로 패킷 전달

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-to-service.gif)



맨 처음 파드의 네트워크 네임스페이스에 있는 `eth0` 인터페이스를 통해 패킷이 파드로부터 떠난다. 그 후 가상 이더넷 장치를 통해 브릿지로 이동한다. 브릿지에 구동 중인 ARP 프로토콜은 서비스에 대해 알지 못하기 때문에 기본 라우트인 `eth0`을 통해 패킷이 빠져나온다. `eth0` 에 빠져나오기 전에, 패킷은 iptables을 통해 필터링된다. 패킷을 받은 후, iptables은 노드에서 kube-proxy에 의해 설정된 규칙을 사용하여 패킷의 목적지(`dst`) 를 서비스 IP에서 특정 파드 IP로 변경한다. 패킷은 이제 서비스의 가상 IP가 아닌 Pod의 IP로 도달하도록 변경된다. 리눅스 커널의 `conntrack` 유틸리티는 추후의 트래픽이 동일한 파드로 라우팅되도록 파드 선택을 기억하기 위해 iptables에 의해 활용된다. 본질적으로 Iptables은 노드에서 직접 클러스터 내 로드밸런싱을 수행하였다. 트래픽은 그 후에 `Pod-to-Pod` 방식으로 전달된다.



### 서비스에서 파드로 패킷 전달

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/service-to-pod.gif)



패킷을 전달받은 파드는 출발지 IP를 자신의 IP로, 목적지 IP를 패킷을 전달한 파드 IP로 하여 응답을 전송할 것이다. 노드에 진입하면 패킷은 iptable을 통해 흐르며 iptables은 `conntrack` 을 사용하여 이전에 선택한 것을 기억하고 패킷의 출발지를 Pod의 IP 대신 서비스의 IP로 재작성한다. 그 후 패킷은 브릿지를 통해 파드의 네임스페이스와 쌍을 이루는 가상 이더넷 장치와 파드의 이더넷 장치로 흐른다.



### DNS 사용

쿠버네티스는 애플리케이션 내에서 서비스의 ClusterIP 주소를 하드코딩하는 것을 방지하기 위해 선택적으로 DNS를 사용할 수 있다. 쿠버네티스 DNS 규칙은 클러스터에서 스케줄링된 일반 쿠버네티스 서비스로 실행된다. 컨테이너가 DNS의 서비스 IP를 사용하여 DNS 이름을 확인하도록 각 노드에서 실행되는 `kubelet` 을 구성한다. 클러스터에 정의된 모든 서비스에 DNS 이름이 할당된다. DNS 레코드는 필요에 따라 서비스의 ClusterIP 또는 파드의 IP로 DNS의 이름을 확인한다. SRV 레코드는 서비스를 실행하기 위한 특정 명명된 포트를 지정하는 데 사용된다.

DNS 파드는 세 가지 개별 컨테이너로 구성된다.

* `kubedns` : 서비스와 엔드포인트의 변경 사항에 대해 쿠버네티스 마스터를 감시하며 DNS 요청을 처리하기 위해 인메모리 룩업 구조를 유지한다.
* `dnsmasq` : 성능 개선을 위해 DNS 캐싱을 추가한다.
* `sidecar` : `dnsmasq` 와 `kubedns` 를 헬스체크하기 위해서 단일 헬스체크 엔드포인트를 제공한다.



DNS 파드 자체는 각 컨테이너가 DNS 항목을 확인할 수 있도록 시작 시 실행 중인 각 컨테이너에 전달되는 static Cluster IP를 사용하여 쿠버네티스 서비스로 노출된다. DNS 엔트리는 인메모리 DNS를 유지하는 `kubedns` 시스템을 통해 확인된다. `etcd` 는 클러스터 상태용 백엔드 스토리지 시스템이고 `kubedns` 는 `etcd` key-value 저장소를 DNS 엔트리로 변환하는 라이브러리를 사용하여 필요할 때 인메모리 DNS 룩업 구조의 상태를 리빌드한다.

CoreDNS는 `kubedns` 와 유사하지만 더 유연하게 만드는 플러그인 아키텍처로 구축된다. 쿠버네티스 1.11 버전에서는 CoreDNS는 쿠버네티스의 기본 DNS 구현체이다.



## Internet - 서비스 간 네트워킹



### Egress - 클러스터에서 인터넷으로의 트래픽

노드에서 인터넷으로의 트래픽 라우팅은 네트워크에 따라 다르며, 실제로 트래픽을 게시하도록 네트워크를 구성하는 방법에 따라 다르다. 

AWS(NHN Cloud도 마찬가지)에서 쿠버네티스 클러스터는 VCP에서 구동하며, 모든 노드는 쿠버네티스 클러스터내로 접근 가능한 Private IP를 할당받는다. 클러스터 외부로 트래픽이 접근 가능하도록 하려면 인터넷 게이트웨이를 VPC에 붙여야 한다. 인터넷 게이트웨이는 두가지 용도로 사용된다.

* 인터넷으로 라우팅 될 트래픽에 대한 VCP 라우팅 테이블 대상 제공
* 공인 IP 주소로 할당된 모든 인스턴스에 대한 NAT(Network Address Translation) 수행, NAT은 노드의 클러스터 내부에서만 사용할 수 있는 Internal IP 주소를 공인 인터넷을 사용할 수 있는 external IP 주소로 변경하는 역할을 한다.

인터넷 게이트웨이가 설치된 상태에서, VM은 트래픽을 인터넷으로 자유롭게 라우팅할 수 있다. 그러나 약간의 문제가 있다. 파드는 파드가 호스팅된 노드와는 다른 자체 IP주소를 가지고 있으며, 인터넷 게이트웨이로의 NAT 변환은 파드가 어떤 VM에서 실행 중인지에 대한 정보가 없기 때문에 (게이트웨이는 컨테이너에 대해 알지 못한다.) 오직 VM의 IP주소로만 작동한다. 



![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/pod-to-internet.gif)

1. 패킷은 Pod1의 네임스페이스에서 시작하여 루트 네임스페이스로 연결되는 veth pair를 통해 루트 네임스페이스로 이동한다. 
2. 루트 네임스페이스에서 패킷의 IP가 브릿지에 연결된 어떠한 네트워크 세그먼트에도 매치가 안되기 때문에 패킷은 브릿지에서 기본 장치로 이동한다.
3. 루트 네임스페이스의 이더넷 장치에 도달하기 전에 iptable은 패킷을 맹글링한다. 이 경우에는, 패킷의 출발지 IP 주소가 파드의 IP 주소이며, 출발지를 파드로 유지하고 싶다면, 게이트웨이 NAT이 VM에 연결된 IP주소만 알기 때문에 인터넷 게이트웨이가 패킷을 거부할 것이다. 이를 해결하기 위한 방법은 iptable이 (패킷 출발지를 수정함으로써) 출발지 NAT을 수행하도록 하는 것이다. 이를 통해 패킷은 파드가 아닌 VM에서 오는 것처럼 보이도록 한다. 
4. 수정된 출발지 IP를 사용하면 패킷이 VM을 떠나 인터넷 게이트웨이에 도달할 수 있다.
5. 인터넷 게이트웨이는 VM 내부 IP에서 외부 IP로 출발지 IP를 다시 쓰는 또 다른 NAT을 수행한다.
6. 마지막으로, 패킷은 공인 인터넷에 도달할 것이다.
7. 패킷이 돌아오는 길에 (Response), 패킷은 동일한 경로를 따르고 모든 출발지 IP 맹글링은 취소(undo)되어 시스템의 각 계층이 이해하는 IP 주소(노드(VM) 수준의 Internal IP 또는 Pod IP) 로 다시 변경된다. 



### Ingress - 인터넷 트래픽을 쿠버네티스로의 트래픽

클러스터로 트래픽을 가져오는 방법은 네트웨크 서택의 다른 부분에서 작동하는 두 가지 솔루션으로 나뉜다.

* Service Loadbalancer
* Ingress Controller



#### Layere 4 Ingress: Loadbalancer

쿠버네티스 서비스를 생성할 때, 함께 사용할 로드밸런서를 선택적으로 지정할 수 있다. 로드밸런서의 구현체는 서비스에 대한 로드밸런서를 생성하는 방법을 알고있는 `cloud controller` 로 부터 제공받는다. 서비스가 생성되면, 로드밸런서의 IP 주소를 알린다. 최종 사용자는 트래픽을 로드밸런서로 전달하여 서비스와 통신을 시작할 수 있다.

AWS에서 로드밸런서는 대상그룹 내의 노드를 인식하고 클러스터의 모든 노드에서 트래픽 균형을 조정한다. 트래픽이 노드에 도달하면, 서비스에 대해 클러스터 전체에 설치된 `Iptables` 규칙은 트래픽이 관심있는 서비스의 파드에 도달하도록 한다. 



#### 로드밸런서에서 서비스로의 패킷

서비스를 배포한다면, 새로운 로드밸런서가 구동 중인 클라우드 로드밸런서에 생성될 것이다. 로드밸런서는 컨테이너에 대해 모르기 때문에, 트래픽이 로드밸런서에 도달하면 클러스터를 구성하는 VM 전체에 분산된다. 각 VM에서 `Iptables` 규칙은 로드밸런서에서 들어오는 트래픽을 올바른 파드로 보낸다. 파드에서 클라이언트로의 응답은 파드의 IP와 함께 반환된다. 하지만 클라이언트는 로드밸런서의 IP 주소를 알아야한다. iptables과 `conntrack` 은 이전에 본 것처럼 반환 경로에서 IP를 올바르게 다시 작성하는데 사용된다.



![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/internet-to-service.gif)



위의 다이어그램은 파드를 호스팅하는 3개의 VM 앞의 네트워크 로드밸런서 장치를 보여준다. 

1. 들어오는 트래픽은 서비스에 대한 로드밸런서로 전달된다.
2. 로드밸런서가 패킷을 받으면, 여러 VM 중 하나를 무작위로 선택하여 트래픽을 보낸다.
3. 위의 그림과 같은 경우처럼 파드가 없는 VM으로 트래픽이 전달될 때, 구동 중인 노드에서의 iptables rule은 `kube-proxy` 를 사용하여 클러스터에 설치된 내부 로드밸런싱 규칙을 사용하여 패킷을 올바른 파드로 보낸다.
4. iptables은 올바른 NAT을 수행하고 패킷을 올바른 파드로 전달한다.



#### L7 Ingress: Ingress Controller

L7 네트워크인 인그레스는 네트워크 스택의 HTTP/HTTPS 프로토콜 범위에서 작동하며 서비스 위에 구축된다. 인그레스를 활성화하는 첫 번째 단계는 쿠버네티스에서 NodePort 타입을 사용하여 서비스에서 포트를 여는 것이다. 서비스의 타입을 NodePort로 설정했다면, 쿠버네티스 마스터는 NodePort로 지정한 서비스의 포트를 특정 포트 범위 중 하나로 할당하고, 각 노드는 해당 포트를 서비스로 프록시한다.  즉, 노드의 포트로 향하는 모든 트래픽은 iptables 규칙을 사용하여 서비스로 포워딩된다. 서비스에서 파드로의 라우팅은 서비스에서 파드로 트래픽을 라우팅할 때 내부 클러스터 로드밸런싱 패턴을 따른다.

노드의 포트를 인터넷으로 노출시키기 위해서는 Ingress 오브젝트를 사용한다. 인그레스는 HTTP 요청을 쿠버네티스 서비스로 매핑하는 고수준의 HTTP 로드밸런서이다. 인그레스 메서드는 쿠버네티스 클라우드 프로바이더 컨트롤러에서 구현하는 방법에 따라 다르다. L4 네트워크 로드밸런서와 같이, HTTP 로드밸런서는 오직 노드의 IP만 이해하기 때문에, 트래픽 라우팅은 `kube-proxy` 에 의해 각 노드에 설치된 iptables 규칙에서 제공하는 내부 로드밸런싱을 활용한다.

AWS 환경에서는 ALB Ingress Controller는 아마존의 L7 애플리케이션 로드밸런서를 사용하는 쿠버네티스 인그레스를 제공한다. 아래 다이어그램은 컨트롤러가 생성하는 AWS 컴포넌트를 상세히 보여주며 또한 Ingress 트래픽이 ALB에서 쿠버네티스 클러스터로 이동하는 경로를 보여준다.

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/ingress-controller-design.png)



1. 인그레스 컨트롤러는 쿠버네티스 API server 로부터 인그레스 이벤트를 감시한다.
2. 요구를 만족하는 인그레스 리소스를 찾았다면, AWS 리소스를 생성하기 시작한다. AWS는 인그레스 자원으로써 ALB를 사용한다.
3. 로드밸런서는 요청을 하나 이상의 등록된 노드로 라우팅하는 데 사용되는 대상 그룹과 함께 작동한다. 대상 그룹은 Ingress 자원에서 설명하는 각 고유 쿠버네티스 서비스에 대해 AWS에서 생성된다.
4. 리스너는 구성하는 프로토콜과 포트를 사용하여 커넥션 요청을 확인하는 ALB 프로세스이다. 리스너는 인그레스 자원 애노테이션에 자세히 설명된 모든 포트에 대해 인그레스 컨트롤러에 의해 생성된다. 
5. 마지막으로 인그레스 리소스에 지정된 각 경로에 대해 대상 그룹 규칙이 생성된다. 이는 특정 경로로의 트래픽이 올바른 쿠버네티스 서비스로 라우팅되는 것을 보장한다.



#### 인그레스에서 서비스로의 패킷

인그레스를 통한 패킷 흐름방식은 로드밸런서와 비슷하다. 가장 큰 차이는 인그레스는 URL의 경로에 대해 알고있고, 인그레스와 노드간의 초기 연결은 각 서비스에 대해 노드에 노출된 포트를 통해 이루어진다.

![img](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/ingress-to-service.gif)



서비스를 배포했다면, 작업 중인 클라우드 프로바이더 업체에서 새로운 인그레스 로드밸런서가 생성된다. 인그레스 로드밸런서는 컨테이너에 대해 알지 못하므로 트래픽이 로드밸런서에 도달하면 서비스에 대해 보급된 포트를 통해 클러스터를 구성하는 VM으로 트래픽이 분산된다. 각 VM에 대해 Iptables 규칙은 로드밸런서에서 올바른 파드로 들어오는 트래픽을 전달한다. 

파드에서 클라이언트로의 응답은 파드의 IP를 반환하지만, 클라이언트는 로드밸런서의 IP주소가 필요하기 때문에, Iptables과 conntrack은 반환 경로에서 IP를 올바르게 재작성하는데 사용된다.

L7 로드밸런서의 장점은 L7 로드밸런서는 HTTP를 잘 알기 때문에 URL과 경로에 대해 잘 알고 있다. 이를 통해 URL 경로별로 서비스 트래픽을 분류할 수 있다. 또한, 일반적으로 HTTP 요청의  `X-Forward-For` 헤더에 original client IP를 제공한다.





























